\documentclass[article]{jss}
\usepackage{natbib}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Clifford Anderson-Bergman\\Gladstone Institutes, UCSF}
\title{\pkg{icenReg}: Regression Models for Interval Censored Data in R}

%% for pretty printing and a nice hypersummary also set:
\Plaintitle{icenReg: Regression Models for Interval Censored Data in R} %% without formatting
%\Shorttitle{\pkg{foo}: A Capitalized Title} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
  Algorithms for fitting a semi-parametric Cox-PH model for interval censored data have existed in the literature for over 15 years at the time of this writing. However, currently there are no reliable \proglang{R} packages for fitting such models for the analysis of real data. The established algorithms are by no means trivial, so it is unreasonable to think that a researcher would rewrite these algorithms to analyze their own data set. To fill this gap, we created the \proglang{R}-package \pkg{icenReg} whose centerpiece is an algorithm for computing the semi-parametric Cox-PH  or proportional odds model based on a combination of established algorithms for this model and similar problems. Standard errors are estimated via bootstrapping, which is set up to take advantage of multiple cores with minimal effort by the user. We also provide a multiple imputations approach, where the censored data is imputed by a fully parametric model and each imputed data set is then fit with a standard Cox-PH model. 
}
\Keywords{interval censoring, Cox-PH, proportional odds, survival analysis, semi-parametric regression, multiple imputations}
%\Plainkeywords{keywords, comma-separated, not capitalized, Java} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Clifford Anderson-Bergman\\
  Gladstone Institutes\\
  University of California, San Francisco\\
  1650 Owens Street\\
  San Francisco, CA\\
  E-mail: \email{cliff.andersonbergman@gladstone.ucsf.edu}\\
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section[intro]{Introduction}

	In the setting of survival analysis, interval censored data occurs when an event time is known only up to an interval. There are two common forms of interval censored data: case I, or current status data, occurs when each subject is observed at a single (random or fixed by design) time, and all that is recorded is whether the event of interest has occurred or not. This results in all subjects being either left or censored. A classic dataset includes mice that are sacrificed at random times and inspected for lung tumors (\cite{mice_tumors}). If tumors were detected, the mice were recorded to be left censored at time of sacrifice. If no tumors were found, they were recorded as right censored. Case II, or general interval censoring, can include left censored, right censored, uncensored and observations that are censored but neither right nor left censored. The last type of censoring can occur if a subject is regularly inspected and all that is known is that the event of interest occurred between inspections. A classic case II interval censored data set includes semi-regular dentist visits by children, with the event of interest being emergence of permanent teeth (\cite{toothStudy}). By selecting the last visit without permanent teeth and the first with permanent teeth, the researchers knew the event time up to an interval. 
	
	Standard parametric models can be used and are fairly straight forward to implement using standard algorithms. Implementations for such regression models can be found in the \proglang{R}-package (\cite{R}) \pkg{survival} (\cite{R_surv}, \cite{R_surv_book}), fit with the \code{survreg} function. However, these models tend to be less popular in the statistical literature. The reason for this is that they are heavily influenced by the choice of parametric model, for which the model inspection can be extremely difficult due to the interval censoring. Because of this, non-parametric models are often preferred. For univariate data, the non-parametric maximum likelihood estimator (NPMLE) is often preferred (\cite{Turnbull_1976}), which can be thought of as a generalization of the Kaplan Meier curves (\cite{KM_curve}) for interval censored data. This can be fit by the function \code{EMICM} in the package \pkg{Icens} (\cite{Icens}, \cite{EMICM}). Alternatively, this can fit by the function \code{computeMLE} in the package \pkg{MLEcens} (\cite{MLEcens}, \cite{SupRedAlg}) which is much faster. However, \code{computeMLE} expects bivariate rather than univariate interval censored data. This can be used for univariate data by including a degenerate second dimension to the data, resulting in the same estimate as \code{EMICM} but much faster. The NPMLE is notoriously inefficient; for case I interval censored data, the convergence rate has been shown to be $n^{1/3}$ (\cite{Case1Con}) instead of the more standard $n^{1/2}$, while for case II the convergence rate has been conjectured to be $n^{1/2} - n^{1/3}$ (\cite{Case2Con1}, \cite{Case2Con2}), depending on the severity of the interval censoring. 
	 
	 For non-parametric comparison of different stratum, a log-rank test can be used (\cite{ic_logrankTests}). However, this is complicated by the fact that the NPMLE is characterized by a large number of parameters, many of which many be on the boundary. Alternatively, permutation tests may be used to compare separate groups. \proglang{R} implementations of both these tests can be found in the \proglang{R}-package \pkg{interval} (\cite{intervalPackage}), called by the function \code{ictest}.  
	 
	For semi-parametric regression modeling of interval censored data, a Cox-PH model (\cite{CoxPH}) can be used (\cite{ic_coxph}). Unlike the case of right censored data, the regression parameters cannot be estimated without estimating baseline survival distribution. The model can still be kept semi-parametric by using the NPMLE as the baseline survival distribution. Interestingly, even though the rate of convergence can be as low as $n^{1/3}$ for survival estimates based on the NPMLE, it has been shown regression coefficients converge at the standard $n^{1/2}$ rate and are asymptotically normal (\cite{ic_coxph_asyp}), allowing for efficient comparisons with the semi-parametric Cox PH model. Inference on the regression parameters can be done using bootstrap standard errors. Alternatively, a semi-parametric proportional odds model can be used (\cite{propOdds1}, \cite{propOdds2}). 

	It was shown that the model can be fit with an ICM algorithm (\cite{basicICM}, \cite{coxphICM}). This algorithm has been implemented in \proglang{R} in the package \pkg{intcox} (\cite{intcox}). However, there are several shortcomings in this package which make it inadequate for general data analysis: first and foremost, the algorithm does appear to be able handle uncensored observations in the data, implying it would not be able to handle general case II interval censoring. We are not sure what causes this problem. Similarly, the algorithm would often immediately fail in the case of discrete censoring, even if all the observation intervals have strictly positive interval lengths. Furthermore, no standard errors are provided for the estimates (although this can be overcome via bootstrapping). Finally, under the standard settings the algorithm prematurely converges quite often, leading to regression estimates that can be off by a significant amount. For example, in a recent general tutorial on interval censored data (\cite{ic_tutorial}), the authors use \code{intcox} on a sample dataset, which results in final log likelihood of -5368.38 and regression parameters 0.288 and 0.316. However, rerunning this analysis, we see that \code{intcox} reports an error after the first iteration. We found that at the MLE, the log likelihood was in fact -5138.57, with regression estimates of 0.322 and 0.337, using the software we provide. Given these issues, we would not consider \pkg{intcox} reliable for general analysis of real data. 
	
	To the best of our knowledge, there is no \proglang{R}-package available for fitting a semi-parametric proportional odds model for interval censored data. 
	 
	 In \pkg{icenReg}, we provide tools intended for analysis of real data. In particular, we present a more reliable and considerably faster algorithm for fitting the semi-parametric Cox-PH model through the function \code{ic_ph}. In addition, we apply the same algorithm to semi-parametric proportional odds model through the function \code{ic_po}. The algorithm uses the established ICM algorithm as a step and also includes a novel combination of existing algorithms for the NPMLE, largely inspired by the cocktail algorithm for the univariate NPMLE (\cite{cocktail}). This more advanced algorithm allows for much more reliable estimation and allows for quick bootstrapping for moderately sized ($n \leq 1,000$ in the case of continuous censoring, much larger for discrete censoring) datasets. For larger datasets, the bootstrap procedure is written to very easily take advantage of multiple cores. In addition, we provide a multiple imputations (\cite{rubinMI}, \cite{panMI}) approach through the \code{impute_ic_ph} function, using \pkg{survival}'s \code{survreg} to fit the prediction model and \code{coxph} to analyze the imputed data. Although the imputation model relies on parametric assumptions to fit the prediction model and thus can introduce bias, for tight censoring intervals the influence of the prediction model is minimal and so this can provide faster robust estimation in the case of relatively small censoring intervals. 
	 
\section[alg1]{Description of algorithm used in \code{ic\_ph} and \code{ic\_po}}

%\section{Likelihood Function for Semi-parametric Cox-PH Model}
	 
	 For characterization of the semi-parametric Cox-PH and proportional odds models, we start from the assumption of independence of the event time and censoring mechanism. We also use the standard interval notation that for subject $i$, $l_i$ and $r_i$ are the left and right side of their ``observation interval" (\emph{i.e.} the interval for which the event is known to have occurred). This allows for left censoring ($l_i = 0$), right censoring ($r_i = \infty$), uncensored observations ($l_i = r_i$) and general interval censoring. From here, we construct the Cox-PH likelihood function as
	 
	 \[
	 \displaystyle \sum_{i = 1}^n \log \left( S_o(l_i)^{e^{X_i \beta} } - S_o(r_{i+})^{e^{X_i \beta}} \right)
	 \]
	 
	 where $S_o$ is the baseline survival function, $X_i$ are an individual's covariates with no intercept and $\beta$ is a vector of coefficients. Similarly, we can define the likelihood function in the proportional odds model as
	 
	  \[
	 \displaystyle \sum_{i = 1}^n \log \left( \frac{S_o(l_i) e^{X_i \beta}}{S_o(l_i) e^{X_i \beta} -S_o(l_i) + 1 } -\frac{S_o(r_i) e^{X_i \beta}}{S_o(r_i) e^{X_i \beta} -S_o(r_i) + 1 }  \right)
	 \]
	 
	 It is worth noting that while for continuous distributions, censored and uncensored observations must be handled differently (\emph{i.e.} with the survival distribution and probability density function respectively), in the case of the semi-parametric estimator, it has been shown that the baseline distribution can treated as a discrete distribution and so $S_o(t) - S_o(t_+) > 0$ if probability mass is placed at $t$. In fact, it has been shown that the MLE places positive probability only inside \emph{Turnbull intervals} (\cite{Turnbull_1976}), $\emph{i.e.}$ intervals such that the left side of the interval is the left side of an observation interval, the right side of the interval is the right side of (possibly a different) observation interval, with no ends of any other intervals in between. Uncensored observations are considered to be intervals of length 0, and so automatically become Turnbull intervals. With this in mind, the above likelihood functions can be written as a function of a finite number of parameters by replacing $	S_o(x_i)  $
with  
\[
1 - \displaystyle \sum_{T_j < x_i} p_j 
\]	 
%	 \[
%	 \displaystyle \sum_{i = 1}^n \log \left[  
%	 	\left(1 - \sum_{T_j < l_i} p_j \right) ^ {exp(X_i \beta)} - \left(1 - \sum_{T_j \leq r_i} p_j \right) ^{exp(X_i \beta)}
%	 \right]
%	 \] 
	 where $p_j$ is the probability mass assigned to Turnbull interval $T_j$. This is a slight abuse of notation, as comparing an interval ($T_j$) to a point ($l_i$ or $r_i$) is typically ill defined. In our case, by definition of the Turnbull intervals, if $l_i$ or $r_i$ (\emph{i.e.} the ends of an observation interval) are greater than or equal to any single point in a Turnbull interval, they are greater than or equal to all points in the Turnbull interval. Likewise for less than or equal to. Under this parameterization, we can characterize the MLE with a finite number of parameters; the regression parameters $\beta$ and baseline distribution probability masses $p$. 

	In the case of the Cox PH model, it has been shown that the likelihood can be maximized by the ICM algorithm (\cite{coxphICM}). In this algorithm, the baseline probabilities and the regression parameters are updated simultaneously by approximating the likelihood function by a second order Taylor expansion (although the off diagonals of the Hessian are ignored) and then maximizing this quadratic function while respecting the linear constraints $p_i > 0$ and $\sum p_i = 1$ via quadratic programming. Half-stepping is used to enforce monotonic convergence.
	
	In our algorithm, we do use an ICM step. However, we only update the baseline parameters in the ICM step, and update the regression parameters separately in a standard conditional Newton-Raphson step. In addition, while the ICM algorithm was a very useful improvement in calculating the univariate NPMLE (a very similar problem to fitting the baseline distribution), it was found that it could be considerably improved by pairing with other algorithms. In particular, the EM/ICM algorithm was shown to do significantly better than either the EM or ICM algorithm alone. In the case of the Cox-PH and proportional odds model, the M step of the standard EM algorithm is not in closed form and so we decided not to investigate that method. Instead, we modeled our algorithm after the cocktail algorithm (\cite{cocktail}), which uses a standard EM step to update all the parameters, a VEM step (\cite{vem}) to exchange probability mass between the parameters with the highest and lowest derivative and a nearest neighbor exchange step. Since the EM algorithm is not available for the Cox-PH model, this is replaced with the ICM step. Likewise, in the traditional cocktail algorithm, optimization of probability mass exchanged in the VEM and nearest neighbor steps is done by the squeezing EM step. Rather than using the squeezing EM step, we use Newton's method with half-stepping to optimize the probability mass exchanged between the parameters. 

	\begin{table}
	\begin{center}
	\begin{tabular}{| c | c | c | c |}
	
	
		\hline
						&	\multicolumn{2}{| c |} {Continuous Censoring}	&		Discrete Censoring	\\
		\hline
						& Mean Time 			&  Mean $\hat{lk} - lk$	&	Mean Time		\\
	\hline
	\multicolumn{4}{| c |}{$ n = 100$}	\\
	\hline
	
	\code{ic_ph}			&	0.009			&		0.000		&	0.008			\\
	
	\code{intcox}: default		&	0.363			&		0.402		&	NA						\\
	
	\code{intcox}: strict		&	2.812			&		0.014		&	NA					\\
	
	\hline
	\multicolumn{4}{| c |}{$ n = 500$}	\\
	\hline
	
	\code{ic_ph}			&	0.058			&		0.000		&	0.062				\\
	
	\code{intcox}: default		&	0.611			&		2.587		&	NA				\\
	
	\code{intcox}: strict		&	7.421			&		0.090		&	NA			\\


	\hline
	\multicolumn{4}{| c |}{$ n = 1,000$}	\\
	\hline
	
	\code{ic_ph}			&		0.198		&		0.000		&	0.131				\\
	
	\code{intcox}: default		&		0.914		&		5.842		&	NA				\\
	
	\code{intcox}: strict		&		16.13		&		0.244		&	NA				\\
	
	\hline

	\multicolumn{4}{| c |}{$ n = 5,000$}	\\
	\hline
	
	\code{ic_ph}			&		31.72		&		0.000		&	1.208			\\
	
	\code{intcox}: default		&		5.033		&		34.51		&	NA	\\
	
	\code{intcox}: strict		&		101.2		&		2.929		&	NA	\\
	
	\hline

	\end{tabular}
	\caption[Average computation times]{Average computation times in seconds and average difference in final log likelihood from MLE. \code{intcox}: default refers to default settings of the \code{intcox} function, while \code{intcox}: strict refers to the same function but with a stricter convergence criteria. For discrete censoring, the \code{intcox} algorithm failed the majority of the time, and so Mean Time for this algorithm was meaningless. Likewise, Mean $\hat{lk} - lk$ was meaningless for discrete censoring, as only the \code{ic\_ph} estimate was calculated. }
	
	\label{table:speedTable}
	\end{center}
	\end{table}
	
	% However, even with this issue, there is little question that \code{ic_ph} outperforms \code{intcox}.
	
	 It is worth noting that the complexity of each step of the algorithm is $O(nm)$, where $n$ is the number of observations and $m$ is the number of maximal intersections. As such, under a continuous inspection mechanism, $m$ typically grows linearly with $n$ and so the complexity of each step is essentially $O(n^2)$. However, with a discrete censoring mechanism, it is typically the case that $m << n$, implying faster computation under a discrete censoring process. %The discrete case is much more common in practice, but for the sake of being thorough, we will consider both cases.  
	
	To examine the speed of the algorithms, we simulated interval censored data (via the function \code{simICPH_beta}) with two covariates and sample sizes $n$ = 100, 500, 1,000 and 5,000, where the true baseline distribution was beta(2,2). We considered both a continuous and discrete censoring mechanism. For the case of continuous inspection, the inspection process was a single uniform(0,1) inspection time. For the discrete inspection process, the inspection time was again a uniform(0,1), but this time rounded to the second decimal, implying $m \leq 100$.

	Fair comparison of the speed of our algorithm with that found in \pkg{intcox} is somewhat confounded by the fact that the algorithm called by \code{intcox} typically prematurely terminates, biasing the speed estimates. For a more complete comparison, we used both the default settings and also supplied a more strict convergence criteria to \code{intcox}. With each dataset, we fit \code{ic_ph}, \code{intcox} with default settings and \code{intcox} with stricter convergence criteria (referred to as \code{intcox}:default and \code{intcox}:strict respectively). For the \code{intcox}:strict, we set the argument \code{epsilon} = $10^{-6}$ (this is refers to the required difference in likelihood between iterations for termination. Default is $10^{-4}$). For each sample size and algorithm configuration, we recorded mean time to algorithm termination and mean difference of final likelihood for each configuration compared with the maximum likelihood of all three configurations. At each configuration, 100 simulated datasets were fit. The algorithm used in \code{intcox} very frequently failed with discrete censoring, reporting algorithm failure in the first iteration over half the time. Because of this, the speed of the \code{intcox} algorithm was not examined for discrete censoring. Results are presented on table \ref{table:speedTable}.  We found \code{ic_ph} significantly outperformed the other configurations in all situations. It appeared that the competitive speed advantage of \code{ic_ph} technically decreased as $n$ increased (in fact, with $n = 5,000$, \code{intcox}:default stops much faster \code{ic_ph}), but this seems to be a result of the stopping criteria of \code{intcox} being too lax and preforming worse as $n$ increases: even for \code{intcox}:strict, at $n = 5,000$, the average difference in final log likelihood implies that the results should not be trusted. All simulations were run on a 2014 Macbook Pro with a 2.2 GHz i7 processor. 
	
	%In the continuous censoring with $n = 5,000$, the relatively speed advantage of \code{ic_ph} seemed was technically heavily reduced; in fact, the standard settings for \code{intcox} was much faster to terminate. However, this seems to be a result of very premature termination of the algorithm rather than computational advantage, as \code{intcox}:default was found to be extremely far from the MLE, while \code{intcox}:strict was several fold slower and still fairly far from the MLE on average. In every single dataset fit, the likelihood of the \code{ic_ph} was higher than for \code{intcox}.
	
	
\section[impute]{Multiple Imputation Model: \code{impute\_ic\_ph}}	
	
	We also include a multiple imputation based approach for fitting a Cox regression model. This implementation used standard routines provided in the \pkg{survival} package. In particular, the function \code{impute_ic_ph} first fits a fully parametric Weibull model via the \pkg{survival} function \code{survreg}. The posterior parameters of this model are then sampled and then used to impute the exact times of the interval censored data. With each imputed dataset, a standard Cox-PH model is fit via the \pkg{survival} function \code{coxph}. 
	
	This imputation model provides a blend between a fully parametric and semi-parametric model. The imputation model is fully parametric and as such, we observed that misspecifying the baseline distribution in the imputation model can lead to bias estimates. However, with interval censored data, the event time is only partially missing. In particular, for subject $i$, we impute the exact time $t_i$ \emph{conditional on} $l_i \leq t_i \leq r_i$. Thus if the interval $(l_i, r_i)$ is relatively narrow, then the assumption of baseline distribution will have little effect on the imputed $t_i$ (or more importantly, the rank of $t_i$). 

{\section[use]{Using \pkg{icenReg}}	
	
	The core functions in \pkg{icenReg} are \code{ic_ph} and \code{impute_ic_ph} for fitting the standard Cox-PH model and the imputation model respectively. Results of the fitted models can be viewed with the standard \code{summary} method. For fits returned by \code{ic_ph}, plots of estimated survival curves can be viewed via the \code{plot} method. Alternatively, the estimated survival curves can be extracted by the \code{getSCurves} function. For the sake of demonstration, \pkg{icenReg} includes the function \code{simICPH_beta}, which simulates proportional hazards data with a beta baseline distribution and \code{mdata}, a real current status data set from \cite{mice_tumors}.
	
	The functions \code{ic_ph} and \code{impute_ic_ph} shares many arguments. In particular, they both include the standard arguments \code{formula} and \code{data}. In the \code{formula} argument, the response must be a \code{Surv} object with \code{type = "interval2"}. These functions also include the arguments \code{useMCores = F}, a logical indicator for whether multiple cores should be used for the imputation or bootstrap step and \code{seed = NULL}, a numeric value for which seed will be passed to \code{set.seed} before the bootstrap or imputation samples are taken. If \code{seed = NULL}, a random seed is generated and passed to \code{set.seed}. The arguments \code{bs_samples = 20} and \code{imps = 100} refer to the number of bootstrap or imputation samples to be taken. The default values are lower than recommended for inference; this is so users can estimate the time needed to run the necessary samples. Unique to \code{impute_ic_ph} are the arguments \code{eta = 1e-10} and \code{rightCenVal = 10000}. This is because the \pkg{survival} function \code{survreg} used for the imputation model cannot handle values of 0 or \code{Inf}, so values less than \code{eta} are replaced with \code{eta} and values greater than \code{rightCenVal} are replaced with \code{rightCenVal}. In some cases, it may be necessary to supply a larger value of \code{rightCenVal}. 
	
	The \code{summary} function is used in the standard manner. For the \code{plot} function for \code{ic_ph} fits, if only the fit is provided, the baseline survival distribution will be plotted. Alternatively, the second argument supplied to \code{plot} can be a new data.frame, with matching variables names to those found in the original dataset. In this case, \code{plot} will plot the survival curve for each set of covariates. A legend will be plotted using the rownames of the data.frame. For custom plots, the function \code{getSCurves} can be used to extract the survival curves, with similar arguments to \code{plot} (\emph{i.e.} first argument is the fit, optional second argument can be a new data.frame). 
	
	For demonstrative purposes, the package includes a function \code{simICPH_beta}, which simulates data from a Cox-PH model with a beta baseline distribution. The arguments accepted are \code{n}, the number of subjects simulated, \code{b1} and \code{b2}, the coefficients for the covariates, \code{inspections = 1}, the number of inspection times (default is one inspection, resulting in current status data), \code{shape1} and \code{shape2}, the parameters of the baseline distribution and \code{rndDigits}, the number of digits that the inspection times will be round to (\code{rndDigits = NULL} implies no rounding). The covariates associated with \code{b1} and \code{b2} are distributed \code{rnorm(n)} and \code{rbinom(n, 1, 0.5) - 0.5} respectively. The reason a beta distribution was chosen is that this will lead to the imputation model assuming a very inappropriate baseline distribution (Weibull) and one can see that if the number of \code{inspections} is low, the imputation model will lead to heavy bias. However, if the number of \code{inspections} is high, we will see the bias is fairly small. We leave implementation of this to the curious reader. 
	
	In order to use parallel computation of either bootstrap or imputation samples, the user must make and register a cluster via \pkg{doParallel}'s \code{makeCluster} and \code{registerDoParallel} (\cite{doParallel}). We demonstrate this in the next section. 
	
{\section{Example Analysis}}
	 For our example, we will use the dataset \code{mdata} provided in our package. This dataset borrows from \cite{mice_tumors}. In this study, RFM mice were sacrificed and examined for lung tumors. If tumors were found, the mouse was considered left censored with the inspection time being equal to age at sacrifice. If no tumors were found, the mouse was considered right censored. Mice were placed in two different environments: conventional environments and germ free environments. In the \code{mdata} dataset, \code{l} represents the left side of the observation interval, \code{u} represents the right side and \code{grp} is a factor representing the environment the mouse was placed in. 

	Below we analyze the data using both \code{ic_ph} and \code{impute_ic_ph}. To begin with, we will use the package \pkg{doParallel} to set up a cluster.

\begin{CodeChunk}
\begin{Code}
> library(doParallel)
> myCluster <- makeCluster(3, type = 'FORK')
> registerDoParallel(myCluster)
\end{Code}
\end{CodeChunk}	
	
	Next, we will load the data and fit the two models.
	
\begin{CodeChunk}
\begin{Code}
data(mdata)
sp_fit <- ic_ph(Surv(l, u, type = 'interval2') ~ grp, 
	data = mdata, bs_samples = 500, useMCores = T)
mi_fit <- impute_ic_ph(Surv(l, u, type = 'interval2') ~ grp, 
	data = mdata, imps = 500, useMCores = T)
\end{Code}
\end{CodeChunk}
	
	We viewed the results of the two fits with the \code{summary} function. 

\begin{CodeChunk}
\begin{Code}
> summary(sp_fit)

Semi Parameteric Cox PH model for interval censored data
Call = 
ic_ph(formula = Surv(l, u, type = "interval2") ~ grp, data = mdata, 
    bs_samples = 500)

    Estimate Exp(Est) Std. Error z-value      p
grp   0.6277    1.873     0.4087   1.536 0.1246

final llk =  -76.53436 
Iterations =  26 
Bootstrap samples =  500 
> summary(mi_fit)

Multiple Imputations Cox PH model for interval censored data
Call = 
impute_ic_ph(formula = Surv(l, u, type = "interval2") ~ grp, 
    data = mdata, imps = 500)

      Estimate Exp(Est) Std. Error z-value       p
grpge   0.7409    2.098     0.3135   2.363 0.01811

number of imputations =  500 
\end{Code}
\end{CodeChunk}

	For visualization, we plot the \code{ic_ph} fit for the two groups. 
	
\begin{CodeChunk}
\begin{Code}
> newdata <- data.frame(grp = c('ce', 'ge'))
> rownames(newdata) = c('Conventional Environment', 'Germ Free Environment')
> plot(sp_fit, newdata)
> sCurves <- getSCurves(sp_fit, newdata)
\end{Code}
\end{CodeChunk}

	We note that the imputation model results in slightly different estimates than the semi-parametric model, and slightly higher standard errors. This is due to the relative wide observational intervals, leading to heavy influence from the imputation model. 
	
	Based on our findings, we would conclude that while it was estimated that mice in the germ free environment were at higher risk to develop tumors than those in the conventional environment, we did not find enough evidence to reject the notation that the hazard rates might be equal. This implies we trusted the results of the semi-parametric model over the parametric imputation model. We decided on this because the observation intervals were relatively wide, and so the parametric imputation model could result in heavy biases. 

\begin{center}
{\begin{figure}
\centering
\includegraphics{mousePlot.pdf}
\caption{Plotted survival curves for \code{mdata} dataset}
\label{plot:tooth}
\end{figure}}
\end{center}	

%{\section[inspect]{Examination of Estimator Characteristics}
%\label{sec:inspect}}

%	Using our new software, we present a very brief investigation of the operating characteristics of our estimators. We address two topics: first, we re-investigate the issues of upward bias of regression coefficients for the semi-parametric model found in Wei Pan 1999. Second, we investigate the sensitivity of the parametric assumption of the multiple imputations model. In both cases, we 	
	
	
{\section[future]{Future Expansions}} 

	We have several plans to expand \pkg{icenReg} to allow for easier analysis of interval censored data. In particular, we would like to provide user-friendly tools for model assessment, such as single function for stratifying the data on different levels of a covariate to examine the proportional hazards assumption. We are also interested in adding a semi-parametric proportional odds model, which should be able to use a similar algorithm as the proportional hazards model. 

%\bibliographystyle{te}	
\bibliography{icenReg}
	
	
\end{document}
